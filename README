README â€“ Created: April 2011
Last Revised: April 2012

I have been working through using tesseract <http://code.google.com/p/tesseract-ocr/>
for OCR processing, with the very handy python-tesseract 
<http://code.google.com/p/python-tesseract/> for pulling it all together.

The main target has been newspaper images, which seems to stretch the limits of 
OCR software and there is some patching to increase parameters that
were problematic for this kind of material, as well as to add some addition
functionality.

The links above include the documentation for the SVN and development
versions of the application. The patches that are in use now are:

# for tesseract
baseapi.h.patch
baseapi.cpp.patch

# for python-tesseract
main_dummy.cpp.patch
main_dummy.h.patch
tesseract.i.patch

Assuming you have the svn versions (last used on tesseract version 719, 
python-tesseract version 100), patching would be a process like:

# for tesseract
patch -p1 api/baseapi.h < /patches/baseapi.h.patch
patch -p1 api/baseapi.cpp < /patches/baseapi.cpp.patch

# for python-tesseract
patch -p1 main_dummy.cpp < /patches/main_dummy.cpp.patch
patch -p1 main_dummy.h < /patches/main_dummy.h.patch
patch -p1 tesseract.i < /patches/tesseract.i.patch

Note that tesseract now uses "tesseract" instead of "tesseract_api"
for the library name.

The python application (ossocr.py) has several parameters for standalone
processing, but is set up by default for hadoop stream processing. For
standalone and testing, you probably want something like this:

python ossocr.py -c True -l eng -f page.jpg

where:

-c
    write coordinate information ("coords.xml" file is created)
-f
    specify input file, use for standalone, non-hadoop processing
-l
    tesseract language code


By default for standalone, the raw OCR goes to a file called "ocr.txt" 
but this can be changed with the -o parameter.

For a large number of files, hadoop is a good option for enlisting
multiple machines to provide better throughput:

hadoop jar /hadoop/contrib/streaming/hadoop-*streaming*.jar -file ./ossocr.py -mapper \
./ossocr.py -file ./reducer.py -reducer ./reducer.py -input /home/hduser/files.txt \
-output /home/hduser/ossocr-output

You can test streaming from the command line:

cat files.txt | python ossocr.py
cat files.txt | python ossocr.py | python reducer.py

To allow some flexibility with files ramped up for OCR processing, a list of
filenames is streamed in ("files.txt" in the example above). This is done for
two reasons. One is that streaming in binary image data is tricky and this 
also allows files to be assembled in a temp directory, a much needed workaround
for processing images in the lab that I had access to for working through hadoop.

The other reason is that files can be specified with an "http" or "https"
prefix, which means they can reside ourside of local storage for processing. When
using Amazon Elastic MapReduce, this gives some flexibility for storing files
outside of Amazon's S3 service.

Like python-tesseract, php-tesseract uses swig <http://www.swig.org/> to
expose tesseract functions to php. The buildall script will 
try to create all of the components but the resulting library
(tesseract.so) will need to be referenced in the appropriate php.ini
file.

The "test.php" shows how the tesseract functions are invoked, for
example:

$mImgFile = "eurotext.tif";
$result=tesseract::ProcessPagesWrapper($mImgFile,$api);
printf("%s\n",$result);

$lenresult = tesseract::ExtractResultsWrapper($api, "coords.txt", strlen($result), "");

The php option is useful for Drupal environments and any of the many other spaces
where php is the hammer of choice.

It is still unclear what the optimum image processing should be
for the most effective OCR, but the gimp script (wavelet.scm)
shows the steps being tried for digital photos of newspaper pages, a combination
that represents a fairly common and challenging scenario for digitization. So far, 
the parameters that seem to be produce the clearest text are:

gimp -i -b "(batch-wavelet-enhance \"*.JPG\" 25.0 2.0 1.0 0.75)" -b "(gimp-quit 0)"

The success of the OCR depends almost entirely on the image being processed. In
general, the contrast between the text and the background media seem to be the key
parameters, scanning pages directly from analogue sources may not require much
manipulation.

This is a work in progress, comments and suggestions welcome. 

art rhyno <http://projectconifer.ca>
